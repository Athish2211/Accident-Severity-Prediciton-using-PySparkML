{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f7aa3bc",
        "outputId": "596696bf-0ce2-47dc-e791-2821ace0ed63"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Motor Vehicle Collisions EDA\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Session initialized.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "badecc2c"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "### Subtask:\n",
        "Load the dataset from '/content/Motor_Vehicle_Collisions_-_Crashes.csv' into a PySpark DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66733d46",
        "outputId": "8bf12bb9-1b5f-4007-dabd-45dfa44ca8c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "source": [
        "csv_file_path = '/content/Motor_Vehicle_Collisions_-_Crashes.csv'\n",
        "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "print(f\"Data loaded successfully from {csv_file_path} into a PySpark DataFrame.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/Motor_Vehicle_Collisions_-_Crashes.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-252696136.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcsv_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/Motor_Vehicle_Collisions_-_Crashes.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Data loaded successfully from {csv_file_path} into a PySpark DataFrame.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/Motor_Vehicle_Collisions_-_Crashes.csv."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MmA3l4q81qVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e08c81b4"
      },
      "source": [
        "## Data Inspection\n",
        "\n",
        "Display the schema and show a sample of the data to understand its structure and content within PySpark.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6df4007b"
      },
      "source": [
        "print(\"DataFrame Schema:\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\nSample of the DataFrame:\")\n",
        "df.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b618d55"
      },
      "source": [
        "## Summary Statistics\n",
        "\n",
        "Generate descriptive statistics for the PySpark DataFrame to get an overview of numerical columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5bb1d41"
      },
      "source": [
        "print(\"Descriptive Statistics for DataFrame:\")\n",
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f54010c9"
      },
      "source": [
        "## Check for Missing Values\n",
        "\n",
        "Identify and count missing values in each column of the PySpark DataFrame to assess data completeness.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0df772a5"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "import pandas as pd\n",
        "\n",
        "missing_values_counts = []\n",
        "\n",
        "for column_name in df.columns:\n",
        "    null_count = df.where(F.col(column_name).isNull()).count()\n",
        "    missing_values_counts.append({'Column Name': column_name, 'Missing Values Count': null_count})\n",
        "\n",
        "missing_values_df = pd.DataFrame(missing_values_counts)\n",
        "\n",
        "print(\"Missing Values Count per Column:\")\n",
        "print(missing_values_df.to_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8e69faf"
      },
      "source": [
        "## Visualize Key Distributions\n",
        "\n",
        "Convert the PySpark DataFrame to a Pandas DataFrame and create relevant visualizations (e.g., histograms for numerical data, bar plots for categorical data) to understand the distribution of important variables. Ensure all plots have appropriate legends.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "459aaa29"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sample_ratio = 0.1\n",
        "pandas_df = df.sample(False, sample_ratio, seed=42).toPandas()\n",
        "print(\"PySpark DataFrame successfully sampled and converted to Pandas DataFrame.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7be8f3d6"
      },
      "source": [
        "plt.figure(figsize=(18, 15))\n",
        "\n",
        "# Plot 1: Distribution of Crashes by Borough\n",
        "plt.subplot(2, 2, 1)\n",
        "borough_counts = pandas_df['BOROUGH'].value_counts().sort_index()\n",
        "sns.lineplot(x=borough_counts.index, y=borough_counts.values, marker='o')\n",
        "plt.title('Distribution of Crashes by Borough')\n",
        "plt.xlabel('Borough')\n",
        "plt.ylabel('Number of Crashes')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot 2: Distribution of Number of Persons Injured\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.histplot(pandas_df['NUMBER OF PERSONS INJURED'].dropna(), bins=range(0, 10), kde=False, color='skyblue')\n",
        "plt.title('Distribution of Number of Persons Injured')\n",
        "plt.xlabel('Number of Persons Injured')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(range(0, 10))\n",
        "\n",
        "# Plot 3: Top 10 Contributing Factors (Vehicle 1)\n",
        "plt.subplot(2, 2, 3)\n",
        "top_10_factors = pandas_df['CONTRIBUTING FACTOR VEHICLE 1'].value_counts().nlargest(10)\n",
        "sns.barplot(x=top_10_factors.index, y=top_10_factors.values,\n",
        "            hue=top_10_factors.index, palette=\"magma\", dodge=False, legend=False)\n",
        "plt.title('Top 10 Contributing Factors (Vehicle 1)')\n",
        "plt.xlabel('Contributing Factor')\n",
        "plt.ylabel('Number of Occurrences')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Plot 4: Top 10 Vehicle Types Involved\n",
        "plt.subplot(2, 2, 4)\n",
        "top_10_vehicle_types = pandas_df['VEHICLE TYPE CODE 1'].value_counts().nlargest(10)\n",
        "sns.barplot(x=top_10_vehicle_types.index, y=top_10_vehicle_types.values,\n",
        "            hue=top_10_vehicle_types.index, palette=\"plasma\", dodge=False, legend=False)\n",
        "plt.title('Top 10 Vehicle Types Involved')\n",
        "plt.xlabel('Vehicle Type')\n",
        "plt.ylabel('Number of Occurrences')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"Visualizations generated successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "357ad4a7"
      },
      "source": [
        "## Prepare Data for Correlation\n",
        "\n",
        "Select numerical columns relevant for correlation analysis, assemble them into a vector column, and handle any missing values in these columns to ensure valid correlation calculation using PySpark.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d14e68f"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "numerical_cols = [\n",
        "    'LATITUDE', 'LONGITUDE',\n",
        "    'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',\n",
        "    'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',\n",
        "    'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
        "    'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED'\n",
        "]\n",
        "\n",
        "df_cleaned = df.na.drop(subset=['LATITUDE', 'LONGITUDE'])\n",
        "\n",
        "for col_name in numerical_cols:\n",
        "    if col_name not in ['LATITUDE', 'LONGITUDE']:\n",
        "        df_cleaned = df_cleaned.na.fill(0, subset=[col_name])\n",
        "\n",
        "print(f\"DataFrame size after dropping nulls in LATITUDE/LONGITUDE and filling others: {df_cleaned.count()} rows\")\n",
        "\n",
        "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features\")\n",
        "\n",
        "df_vector = assembler.transform(df_cleaned)\n",
        "\n",
        "print(\"Schema of DataFrame with vector column:\")\n",
        "df_vector.printSchema()\n",
        "\n",
        "print(\"Sample of DataFrame with new 'features' vector column:\")\n",
        "df_vector.select(numerical_cols + [\"features\"]).show(5, truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13dddd50"
      },
      "source": [
        "from pyspark.ml.stat import Correlation\n",
        "\n",
        "correlation_matrix = Correlation.corr(df_vector, \"features\").head()\n",
        "\n",
        "print(\"Correlation Matrix:\")\n",
        "print(correlation_matrix[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80d3224d"
      },
      "source": [
        "## Visualize Correlation Heatmap\n",
        "\n",
        "Convert the PySpark correlation matrix to a Pandas DataFrame and use `seaborn.heatmap` to visualize the correlations. This heatmap will help identify which variables are strongly correlated with 'NUMBER OF PEDESTRIANS KILLED' and could be good candidates for model features. Ensure the plot has a clear title, axis labels, and color bar.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1d3b7b2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "corr_matrix_dense = correlation_matrix[0]\n",
        "\n",
        "corr_matrix_np = corr_matrix_dense.toArray()\n",
        "\n",
        "correlation_df = pd.DataFrame(corr_matrix_np, index=numerical_cols, columns=numerical_cols)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_df, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)\n",
        "\n",
        "plt.title('Correlation Matrix of Numerical Features', fontsize=16)\n",
        "\n",
        "plt.xlabel('Features', fontsize=12)\n",
        "plt.ylabel('Features', fontsize=12)\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Correlation heatmap generated successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Model\n",
        "\n",
        "## Data Preparation\n",
        "A new column named \"Severity\" is built where the severity of accident is rated between 0,1 and 2.\n",
        "\n",
        "0 -> No injuries or Deaths\n",
        "\n",
        "1 -> Injuries\n",
        "\n",
        "2 -> Deaths"
      ],
      "metadata": {
        "id": "k0vSYsIRrS3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_vector = df_vector.withColumn(\n",
        "    \"SEVERITY\",\n",
        "    F.when(\n",
        "        (F.col(\"NUMBER OF PERSONS KILLED\") > 0), 2\n",
        "    ).when(\n",
        "        (F.col(\"NUMBER OF PERSONS INJURED\") > 0), 1\n",
        "    ).otherwise(0)\n",
        ")\n",
        "\n",
        "df_vector.select(\"NUMBER OF PERSONS INJURED\",\n",
        "          \"NUMBER OF PERSONS KILLED\",\n",
        "          \"SEVERITY\").show(10)"
      ],
      "metadata": {
        "id": "HmXSRMgIhQ5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting the existing format of the crash date and time into usable ones"
      ],
      "metadata": {
        "id": "n-lvgo5GsDTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import hour, dayofweek, month, col as F_col\n",
        "\n",
        "df = df.withColumn(\"CRASH_HOUR\", hour(F_col(\"CRASH TIME\")))\n",
        "df = df.withColumn(\"CRASH_DAYOFWEEK\", dayofweek(F_col(\"CRASH DATE\")))\n",
        "df = df.withColumn(\"CRASH_MONTH\", month(F_col(\"CRASH DATE\")))"
      ],
      "metadata": {
        "id": "cX9GbDwGiqr0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "54451716-e865-4093-b143-3baac54e60c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionRefusedError",
          "evalue": "[Errno 111] Connection refused",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-820040005.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdayofweek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF_col\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CRASH_HOUR\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CRASH TIME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CRASH_DAYOFWEEK\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdayofweek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CRASH DATE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CRASH_MONTH\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CRASH DATE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFuncT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mcol\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mColumn\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_invoke_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \"\"\"\n\u001b[1;32m     95\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mjf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_jvm_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m_get_jvm_function\u001b[0;34m(name, sc)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[1;32m     86\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1710\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m         answer = self._gateway_client.send_command(\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We replace null values with the term \"Unknown\" for categorical variables and 0 for numerical"
      ],
      "metadata": {
        "id": "AaHwsrEesT5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Severity target\n",
        "df = df.withColumn(\n",
        "    \"SEVERITY\",\n",
        "    F.when(F.col(\"NUMBER OF PERSONS KILLED\") > 0, 2)\n",
        "     .when(F.col(\"NUMBER OF PERSONS INJURED\") > 0, 1)\n",
        "     .otherwise(0)\n",
        ")\n",
        "\n",
        "df = df.fillna({\"BOROUGH\": \"UNKNOWN\",\n",
        "                \"CONTRIBUTING FACTOR VEHICLE 1\": \"Unknown\",\n",
        "                \"VEHICLE TYPE CODE 1\": \"Unknown\",\n",
        "                \"VEHICLE TYPE CODE 2\": \"Unknown\"})\n",
        "df = df.fillna(0)   # numeric nulls"
      ],
      "metadata": {
        "id": "j93MAlwyy7U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_cols = [\n",
        "    \"BOROUGH\",\n",
        "    \"CONTRIBUTING FACTOR VEHICLE 1\",\n",
        "    \"VEHICLE TYPE CODE 1\",\n",
        "    \"VEHICLE TYPE CODE 2\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "M8IyBKwUzcRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StringIndexer transforms categorical string fields into numeric indices by assigning each unique category a distinct integer. This enables machine-learning models—which require numerical inputs—to properly interpret categorical variables. Using handleInvalid='keep' ensures unseen or null categories are safely mapped instead of causing errors."
      ],
      "metadata": {
        "id": "0acL0Vb4tT_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=col, outputCol=col + \"_IDX\", handleInvalid=\"keep\")\n",
        "    for col in categorical_cols\n",
        "]\n"
      ],
      "metadata": {
        "id": "B82Ix0YvzzCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols = [\n",
        "    \"LATITUDE\", \"LONGITUDE\",\n",
        "    \"CRASH_HOUR\", \"CRASH_DAYOFWEEK\", \"CRASH_MONTH\"\n",
        "]"
      ],
      "metadata": {
        "id": "fDBQr8mHz1or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VectorAssembler combines multiple input feature columns into a single vector column called \"features\", creating the unified numeric feature representation required by Spark ML models for training and prediction."
      ],
      "metadata": {
        "id": "tYCCYkV6tcLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "feature_cols = numerical_cols + [col + \"_IDX\" for col in categorical_cols]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols,\n",
        "    outputCol=\"features\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "_u7oKNcqz4J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Model"
      ],
      "metadata": {
        "id": "iTjxC_t-tfvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"SEVERITY\",\n",
        "    maxIter=50,\n",
        "    regParam=0.01\n",
        ")\n"
      ],
      "metadata": {
        "id": "SU46HoKpz5_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline(stages=indexers + [assembler, lr])\n",
        "\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
        "model = pipeline.fit(train)\n",
        "pred = model.transform(test)"
      ],
      "metadata": {
        "id": "HBa8KOapz8H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance metrics:\n",
        "\n",
        "We've used metrics like Accuracy, F1, Precision and Recall"
      ],
      "metadata": {
        "id": "r2axifwitrLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"SEVERITY\", predictionCol=\"prediction\")\n",
        "\n",
        "acc = evaluator.evaluate(pred, {evaluator.metricName: \"accuracy\"})\n",
        "f1 = evaluator.evaluate(pred, {evaluator.metricName: \"f1\"})\n",
        "prec = evaluator.evaluate(pred, {evaluator.metricName: \"weightedPrecision\"})\n",
        "rec = evaluator.evaluate(pred, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Weighted Precision:\", prec)\n",
        "print(\"Weighted Recall:\", rec)"
      ],
      "metadata": {
        "id": "DCLsrfAO0UEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred.groupBy(\"SEVERITY\", \"prediction\").count().orderBy(\"SEVERITY\", \"prediction\").show()\n",
        "confusion_df = pred.groupBy(\"SEVERITY\") \\\n",
        "                   .pivot(\"prediction\") \\\n",
        "                   .count() \\\n",
        "                   .fillna(0) \\\n",
        "                   .orderBy(\"SEVERITY\")\n",
        "\n",
        "confusion_df.show()"
      ],
      "metadata": {
        "id": "ntOnnNNTOdY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternate Model\n",
        "\n",
        "XGBoost Classifier"
      ],
      "metadata": {
        "id": "PW4rYVPt1OSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost.spark import SparkXGBClassifier\n",
        "\n",
        "xgb = SparkXGBClassifier(\n",
        "    features_col=\"features\",\n",
        "    label_col=\"SEVERITY\",\n",
        "    numClass=3,\n",
        "    maxDepth=8,\n",
        "    eta=0.1,\n",
        "    numRound=150,\n",
        "    subsample=0.8,\n",
        "    colsampleByTree=0.8\n",
        ")"
      ],
      "metadata": {
        "id": "Ok5wU_BpwX6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline2 = Pipeline(stages=indexers + [assembler, xgb])\n",
        "\n",
        "model2 = pipeline2.fit(train)\n",
        "pred2 = model2.transform(test)\n"
      ],
      "metadata": {
        "id": "PgktnIlByxsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"SEVERITY\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "\n",
        "accuracy = evaluator.evaluate(pred2, {evaluator.metricName: \"accuracy\"})\n",
        "f1 = evaluator.evaluate(pred2, {evaluator.metricName: \"f1\"})\n",
        "prec = evaluator.evaluate(pred2, {evaluator.metricName: \"weightedPrecision\"})\n",
        "rec = evaluator.evaluate(pred2, {evaluator.metricName: \"weightedRecall\"})\n",
        "\n",
        "print(\"XGBoost Accuracy:\", accuracy)\n",
        "print(\"XGBoost F1 Score:\", f1)\n",
        "print(\"Weighted Precision:\", prec)\n",
        "print(\"Weighted Recall:\", rec)"
      ],
      "metadata": {
        "id": "3Dkrw8ray3hj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}